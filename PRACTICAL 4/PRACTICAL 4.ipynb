{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natsort in c:\\users\\hbhak\\anaconda3\\lib\\site-packages (8.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hbhak\\anaconda3\\lib\\site-packages (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'README_BASE.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2b073a3e061a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'README_BASE.md'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[0mreadme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0mreadme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{{{word-embedding-table}}}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_word_embedding_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'README_BASE.md'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from operator import attrgetter, itemgetter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "arxiv_prefix = 'https://arxiv.org/abs/'\n",
    "arxiv_prefix_len = len(arxiv_prefix)\n",
    "github_prefix = 'https://github.com/'\n",
    "github_prefix_len = len(github_prefix)\n",
    "arxiv_papers = set()\n",
    "\n",
    "def get_and_sort_meta_info(json_file):\n",
    "    with open(json_file) as f:\n",
    "        meta_info = json.load(f)\n",
    "    meta_info = natsorted(meta_info, key=itemgetter('paper_link'))\n",
    "    with open(json_file, 'w') as f:\n",
    "        f.write(json.dumps(meta_info, indent=4))\n",
    "    return meta_info\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def fancy_code(code, no_attrs=True):\n",
    "    if no_attrs:\n",
    "        attrs = ''\n",
    "    else:\n",
    "        attrs = []\n",
    "        if 'unofficial' in code:\n",
    "            attrs.append('unofficial')\n",
    "        else:\n",
    "            if 'pretrained' in code and not code['pretrained']:\n",
    "                attrs.append('not pretrained')\n",
    "        if 'load_pretrained' in code:\n",
    "            attrs.append('load pretrained')\n",
    "        if 'pretrained' in code and 'unofficial' in code:\n",
    "            attrs.append('pretrained')\n",
    "        if 'email_for_pretrained' in code:\n",
    "            attrs.append('email for pretrained')\n",
    "        if 'no_training_code' in code:\n",
    "            attrs.append('no training code')\n",
    "        if len(attrs) == 0:\n",
    "            attrs = ''\n",
    "        else:\n",
    "            attrs = '({})'.format(', '.join(attrs))\n",
    "    if code['link'].startswith(github_prefix):\n",
    "        tmp = code['link'][github_prefix_len:]\n",
    "        if tmp.endswith('/'):\n",
    "            tmp = tmp[:-1]\n",
    "        if len(re.findall('/', tmp)) > 2:\n",
    "            tmp = tmp[:find_nth(tmp, '/', 2)]\n",
    "        github_stars = ' ![](https://img.shields.io/github/stars/{}.svg?style=social )'.format(tmp)\n",
    "    else:\n",
    "        github_stars = ''\n",
    "    return '[{training_language}]({code_link} ){attrs}{github_stars}'.format(\n",
    "        training_language=code['language'], code_link=code['link'], attrs=attrs, github_stars=github_stars)\n",
    "\n",
    "\n",
    "def query_semantic_scholar(query):\n",
    "    if query == '':\n",
    "        return 'N/A', '-'\n",
    "    try:\n",
    "        global arxiv_papers\n",
    "        if query in arxiv_papers:\n",
    "            raise ValueError('Duplicate Paper {}'.format(query))\n",
    "        arxiv_papers.add(query)\n",
    "        res = json.loads(urllib.request.urlopen(\"https://api.semanticscholar.org/v1/paper/\" + query).read())\n",
    "        count = len(res['citations'])\n",
    "        return (str(count) if count < 999 else '999+'), str(res['year']) + '/??'\n",
    "    except:\n",
    "        return 'N/A', '-'\n",
    "\n",
    "\n",
    "def fetch_common_parts(paper):\n",
    "    is_arxiv = paper['paper_link'].startswith(arxiv_prefix)\n",
    "    if is_arxiv:\n",
    "        arxiv_id = paper['paper_link'][arxiv_prefix_len:]\n",
    "        arxiv_date = arxiv_id.split('.')[0]\n",
    "        date_part = '20' + arxiv_date[:2] + '/' + arxiv_date[2:]\n",
    "        citation_part, _ = query_semantic_scholar('arXiv:{}'.format(arxiv_id))\n",
    "    else:\n",
    "        citation_part, date_part = query_semantic_scholar(paper.get('doi', paper.get('s2_paper_id', '')))\n",
    "    paper_part = '[{paper_title}]({paper_link})'.format(paper_title=paper['paper_title'],\n",
    "                                                        paper_link=paper['paper_link'])\n",
    "    return citation_part, date_part, paper_part\n",
    "\n",
    "\n",
    "def generate_word_embedding_table():\n",
    "    header = ['|date|paper|citation count|training code|pretrained models|',\n",
    "              '|:---:|:---:|:---:|:---:|:---:|']\n",
    "    generated_lines = []\n",
    "    meta_info = get_and_sort_meta_info('word-embedding.json')\n",
    "    for paper in tqdm(meta_info, desc='word embedding'):\n",
    "        citation_part, date_part, paper_part = fetch_common_parts(paper)\n",
    "        if 'code' in paper:\n",
    "            training_code_part = fancy_code(paper['code'][0])\n",
    "        else:\n",
    "            training_code_part = '-'\n",
    "        pretrained_part = '-' if 'name' not in paper else '[{name}]({pretrained_link} ){broken_link}' \\\n",
    "            .format(name=paper['name'], pretrained_link=paper['pretrained_link'],\n",
    "                    broken_link='(broken)' if paper.get('broken_link', False) else '')\n",
    "        generated_lines.append(\n",
    "            AttrDict(date_part=date_part, paper_part=paper_part,\n",
    "                     training_code_part=training_code_part,\n",
    "                     pretrained_part=pretrained_part, citation_part=citation_part))\n",
    "    generated_lines = sorted(generated_lines, key=attrgetter('date_part', 'citation_part'))\n",
    "    generated_lines = [\n",
    "        '|{date_part}|{paper_part}|{citation_part}|{training_code_part}|{pretrained_part}|'.format(**x) for x in\n",
    "        generated_lines]\n",
    "    return '\\n'.join(header + generated_lines)\n",
    "\n",
    "\n",
    "def generate_contextualized_table():\n",
    "    header = ['|date|paper|citation count|code|pretrained models|',\n",
    "              '|:---:|:---:|:---:|:---:|:---:|']\n",
    "    generated_lines = []\n",
    "    meta_info = get_and_sort_meta_info('contextualized.json')\n",
    "    for paper in tqdm(meta_info, desc='contextualized'):\n",
    "        citation_part, date_part, paper_part = fetch_common_parts(paper)\n",
    "        if 'code' in paper:\n",
    "            training_code_part = '<br>'.join([fancy_code(code) for code in paper['code']])\n",
    "        else:\n",
    "            training_code_part = '-'\n",
    "        if 'pretrained_models' in paper:\n",
    "            if len(paper['pretrained_models']) == 1:\n",
    "                pretrained_models = '[{name}]({pretrained_link} )'.format(name=paper['model_name'],\n",
    "                                                                          pretrained_link=paper['pretrained_models'][0][\n",
    "                                                                              'link'])\n",
    "            else:\n",
    "                pretrained_links = ', '.join(\n",
    "                    ['[{name}]({link})'.format(name=x['name'], link=x['link']) for x in paper['pretrained_models']])\n",
    "                pretrained_models = '{name}({pretrained_link})'.format(name=paper['model_name'],\n",
    "                                                                       pretrained_link=pretrained_links)\n",
    "        else:\n",
    "            pretrained_models = '-'\n",
    "        generated_lines.append(\n",
    "            AttrDict(date_part=date_part, paper_part=paper_part, training_code_part=training_code_part,\n",
    "                     pretrained_models=pretrained_models, citation_part=citation_part))\n",
    "    generated_lines = sorted(generated_lines, key=attrgetter('date_part', 'citation_part'))\n",
    "    generated_lines = [\n",
    "        '|{date_part}|{paper_part}|{citation_part}|{training_code_part}|{pretrained_models}|'.format(**x) for x in\n",
    "        generated_lines]\n",
    "    return '\\n'.join(header + generated_lines)\n",
    "\n",
    "\n",
    "def generate_encoder_table():\n",
    "    header = ['|date|paper|citation count|code|model_name|',\n",
    "              '|:---:|:---:|:---:|:---:|:---:|']\n",
    "    generated_lines = []\n",
    "    meta_info = get_and_sort_meta_info('encoder.json')\n",
    "    for paper in tqdm(meta_info, 'encoder'):\n",
    "        citation_part, date_part, paper_part = fetch_common_parts(paper)\n",
    "        if 'code' in paper:\n",
    "            training_code_part = '<br>'.join([fancy_code(code) for code in paper['code']])\n",
    "        else:\n",
    "            training_code_part = '-'\n",
    "        model_name = paper['name']\n",
    "        generated_lines.append(\n",
    "            AttrDict(date_part=date_part, paper_part=paper_part, training_code_part=training_code_part,\n",
    "                     model_name=model_name, citation_part=citation_part))\n",
    "    generated_lines = sorted(generated_lines, key=attrgetter('date_part', 'citation_part'))\n",
    "    generated_lines = [\n",
    "        '|{date_part}|{paper_part}|{citation_part}|{training_code_part}|{model_name}|'.format(**x) for x in\n",
    "        generated_lines]\n",
    "    return '\\n'.join(header + generated_lines)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('README_BASE.md', 'r') as f:\n",
    "        readme = f.read()\n",
    "    readme = readme.replace('{{{word-embedding-table}}}', generate_word_embedding_table())\n",
    "    readme = readme.replace('{{{contextualized-table}}}', generate_contextualized_table())\n",
    "    readme = readme.replace('{{{encoder-table}}}', generate_encoder_table())\n",
    "    with open('README.md', 'w',encoding=\"utf-8\") as f:\n",
    "        f.write(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
